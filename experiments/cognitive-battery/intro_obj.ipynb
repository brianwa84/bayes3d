{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitive Battery Introduction: Jax-3DP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to visualize\n",
    "# jax3dp3.setup_visualizer()\n",
    "# jax3dp3.show_cloud(\"c1\", object_cloud)\n",
    "# jax3dp3.show_trimesh(\"mesh\", learned_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax3dp3\n",
    "from jax3dp3.viz import (\n",
    "    save_depth_image,\n",
    "    get_depth_image,\n",
    "    multi_panel,\n",
    ")\n",
    "from jax3dp3.transforms_3d import transform_from_pos, depth_to_coords_in_camera\n",
    "from jax3dp3.jax_rendering import (\n",
    "    get_rectangular_prism_shape,\n",
    "    render_planes_multiobject,\n",
    "    batched_scorer_parallel_params,\n",
    ")\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from jax3dp3.viz import make_gif_from_pil_images\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cog_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize camera metadata and path to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"gravity\"\n",
    "data_path = f\"/home/khaledshehada/cog_jax3dp3_data/{scene}_data/videos/\"\n",
    "num_frames = len(os.listdir(os.path.join(data_path, \"frames\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing frame buffer size to (width, height, depth) = (320, 320, 2048)\n"
     ]
    }
   ],
   "source": [
    "width = 300\n",
    "height = 300\n",
    "fov = 90\n",
    "\n",
    "fx, fy, cx, cy = utils.get_camera_intrinsics(width, height, fov)\n",
    "\n",
    "fx_fy = jnp.array([fx, fy])\n",
    "cx_cy = jnp.array([cx, cy])\n",
    "near, far = 0.001, 50.0\n",
    "K = jnp.array(\n",
    "    [\n",
    "        [fx_fy[0], 0.0, cx_cy[0]],\n",
    "        [0.0, fx_fy[1], cx_cy[1]],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "jax3dp3.setup_renderer(height, width, fx, fy, cx, cy, near, far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load ground-truth RGB images, depth, and segmentation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_images, depth_images, seg_maps = [], [], []\n",
    "rgb_images_pil = []\n",
    "for i in range(num_frames):\n",
    "    rgb_path = os.path.join(data_path, f\"frames/frame_{i}.jpeg\")\n",
    "    if not os.path.isfile(rgb_path):\n",
    "        rgb_path = rgb_path.replace(\"jpeg\", \"png\")\n",
    "    rgb_img = Image.open(rgb_path)\n",
    "    rgb_images_pil.append(rgb_img)\n",
    "    rgb_images.append(np.array(rgb_img))\n",
    "\n",
    "    depth_path = os.path.join(data_path, f\"depths/frame_{i}.npy\")\n",
    "    depth_npy = np.load(depth_path)\n",
    "    depth_images.append(depth_npy)\n",
    "\n",
    "    seg_map = np.load(os.path.join(data_path, f\"segmented/frame_{i}.npy\"))\n",
    "    seg_maps.append(seg_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mask the depth and segmentation images to only include the relevant part of the scene (i.e. crop to the box above table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_images = []  # depth data in 2d view as images\n",
    "seg_images = []  # segmentation data as images\n",
    "\n",
    "for frame_idx in range(num_frames):\n",
    "    coord_image, _ = depth_to_coords_in_camera(depth_images[frame_idx], K)\n",
    "    segmentation_image = seg_maps[frame_idx].copy()\n",
    "    mask = np.invert(\n",
    "        # (coord_image[:, :, 0] < 2.0)\n",
    "        # * (coord_image[:, :, 0] > -1)\n",
    "        # * (coord_image[:, :, 1] < 0.463)\n",
    "        # * (coord_image[:, :, 1] > -0.8)\n",
    "        # * (coord_image[:, :, 2] < 3)\n",
    "        # * (coord_image[:, :, 2] > 0.25)\n",
    "        (coord_image[:, :, 0] < 1.1)\n",
    "        * (coord_image[:, :, 0] > -1)\n",
    "        * (coord_image[:, :, 1] < 0.565)\n",
    "        * (coord_image[:, :, 1] > -1)\n",
    "        * (coord_image[:, :, 2] > 1.2)\n",
    "        * (coord_image[:, :, 2] < 1.35)\n",
    "    )\n",
    "    coord_image[mask, :] = 0.0\n",
    "    segmentation_image[mask, :] = 0.0\n",
    "    coord_images.append(coord_image)\n",
    "    seg_images.append(segmentation_image)\n",
    "\n",
    "coord_images = np.stack(coord_images)\n",
    "seg_images = np.stack(seg_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "tube_s_long\n",
      "bowl\n",
      "tube_s\n"
     ]
    }
   ],
   "source": [
    "# Load meshes\n",
    "meshes = []\n",
    "meshes_path = \"/home/khaledshehada/cog_jax3dp3_data/gravity_data/meshes/\"\n",
    "for mesh_name in os.listdir(meshes_path):\n",
    "    mesh_path = os.path.join(meshes_path, mesh_name)\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    jax3dp3.load_model(mesh)\n",
    "    meshes.append(mesh_name.replace(\".obj\", \"\"))\n",
    "    print(meshes[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pick a starting frame and initialize the object shapes and poses from that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = 36\n",
    "seg_img = seg_images[start_t]\n",
    "# Image.fromarray(seg_img)\n",
    "\n",
    "Image.fromarray(seg_img).save(\"seg_img.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=300x300>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_objects = 12\n",
    "indices, init_poses = [], []\n",
    "obj_ids = jnp.unique(seg_img.reshape(-1, 3), axis=0)\n",
    "obj_ids = sorted(\n",
    "    obj_ids, key=lambda x: jnp.all(seg_img == x, axis=-1).sum(), reverse=True\n",
    ")\n",
    "for obj_id in obj_ids[: num_objects + 1]:\n",
    "    if jnp.all(obj_id == 0):\n",
    "        # Background\n",
    "        continue\n",
    "\n",
    "    obj_mask = jnp.all(seg_img == obj_id, axis=-1)\n",
    "    masked_depth = coord_images[start_t].copy()\n",
    "    masked_depth[~obj_mask] = 0\n",
    "\n",
    "    object_points = coord_images[start_t][obj_mask]\n",
    "    maxs = np.max(object_points, axis=0)\n",
    "    mins = np.min(object_points, axis=0)\n",
    "    dims = maxs - mins\n",
    "    obj_center = (maxs + mins) / 2\n",
    "    obj_transform = transform_from_pos(obj_center)\n",
    "\n",
    "    best = None\n",
    "    k = np.inf\n",
    "    for m in range(len(meshes)):\n",
    "        obj_transforms = utils.get_object_transforms(meshes[m], obj_transform)\n",
    "        for i, transform in enumerate(obj_transforms):\n",
    "            rendered_image = jax3dp3.render_single_object(transform, m)\n",
    "            keep_points = (\n",
    "                jnp.sum(\n",
    "                    jnp.logical_or(\n",
    "                        (\n",
    "                            (masked_depth[:, :, 2] != 0.0)\n",
    "                            * (rendered_image[:, :, 2] == 0)\n",
    "                        ),\n",
    "                        (\n",
    "                            (masked_depth[:, :, 2] == 0.0)\n",
    "                            * (rendered_image[:, :, 2] != 0)\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "                / (rendered_image[:, :, 2] != 0.0).sum()\n",
    "            )\n",
    "            if keep_points < k:\n",
    "                k = keep_points\n",
    "                best = (m, transform)\n",
    "    if best:\n",
    "        indices.append(best[0])\n",
    "        init_poses.append(best[1])\n",
    "\n",
    "init_poses = jnp.array(init_poses)\n",
    "rendered_image = jax3dp3.render_multiobject(init_poses, indices)\n",
    "get_depth_image(rendered_image[:, :, 2], max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the liklihood methods and the proposal enumerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liklihood parameters\n",
    "r = radius = 0.1\n",
    "outlier_prob = 0.01\n",
    "outlier_volume = 10\n",
    "\n",
    "# Enumeration parameters\n",
    "n = 7  # number of enumerated proposals on each dimension (x, y, z).\n",
    "d = 0.05  # the minimum and maximum position delta on each dimension (x, y, z).\n",
    "\n",
    "# Enumerating proposals\n",
    "translation_deltas = jax3dp3.make_translation_grid_enumeration(\n",
    "    -d, -d, -d, d, d, d, n, n, n\n",
    ")\n",
    "\n",
    "\n",
    "def scorer(rendered_image, gt, prior):\n",
    "    weight = jax3dp3.likelihood.threedp3_likelihood(\n",
    "        gt, rendered_image, r, outlier_prob, outlier_volume\n",
    "    )\n",
    "    return prior * weight\n",
    "\n",
    "\n",
    "scorer_parallel = jax.vmap(scorer, in_axes=(0, None, 0))\n",
    "scorer_parallel_jit = jax.jit(scorer_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea for liklihood model under occlusion:\n",
    "# render object, check for k closest points in the point cloud, minimize the distance between current rendering and distance from closest points\n",
    "pose_estimates = init_poses.copy()\n",
    "t = start_t\n",
    "gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "translation_deltas_full = jnp.tile(\n",
    "    jnp.eye(4)[None, :, :],\n",
    "    (translation_deltas.shape[0], pose_estimates.shape[0], 1, 1),\n",
    ")\n",
    "translation_deltas_full = translation_deltas_full.at[:, 0, :, :].set(translation_deltas)\n",
    "translation_proposals = jnp.einsum(\n",
    "    \"bij,abjk->abik\", pose_estimates, translation_deltas_full\n",
    ")\n",
    "images = jax3dp3.render_parallel(translation_proposals, 0)\n",
    "# prior = 1 + translation_deltas[:,1,3] / n\n",
    "prior = jnp.ones((translation_deltas.shape[0],))\n",
    "weights_new = scorer_parallel_jit(images, gt_image, prior)\n",
    "pose_estimates = translation_proposals[jnp.argmax(weights_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:29<00:00,  5.07it/s]\n"
     ]
    }
   ],
   "source": [
    "num_steps = num_frames - start_t\n",
    "num_steps = 150\n",
    "inferred_poses = []\n",
    "occlusion_threshold = 10\n",
    "pose_estimates = init_poses.copy()\n",
    "# tqdm = lambda x: x\n",
    "# occluded = False\n",
    "for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "    gt_image = jnp.array(coord_images[t])\n",
    "    # New objects\n",
    "    n_objects = pose_estimates.shape[0]\n",
    "    for i in range(n_objects - 1, n_objects):\n",
    "        # Check for occlusion\n",
    "        depth_wi = jax3dp3.render_multiobject(pose_estimates, indices)\n",
    "        depth_woi = jax3dp3.render_multiobject(pose_estimates[:i], indices[:i])\n",
    "        if jnp.sum(depth_wi[:, :, 2] != depth_woi[:, :, 2]) < occlusion_threshold:\n",
    "            # pose_estimate = pose_estimates[i]\n",
    "            # translation_proposals = jnp.einsum(\"ij,ajk->aik\", pose_estimate, translation_deltas)\n",
    "            # pose_estimate = translation_proposals[best_weight_idx]\n",
    "            # pose_estimates = pose_estimates.at[i].set(pose_estimate)\n",
    "            # prior = (1 - translation_deltas[:,1,3]) * abs(translation_deltas[:,0,3]) * abs(translation_deltas[:,2,3])\n",
    "            prior = (1 - translation_deltas[:, 1, 3]) * abs(translation_deltas[:, 2, 3])\n",
    "            # occluded = True\n",
    "            # prior = jnp.mean(jnp.array([\n",
    "            #     1 - jnp.minimum(translation_deltas[:,1,3], 0),\n",
    "            # 1 - jnp.abs(translation_deltas[:,0,3]),\n",
    "            # 1 - jnp.abs(translation_deltas[:,2,3]),\n",
    "            # ]), axis=0)\n",
    "            # prior = jnp.ones((translation_deltas.shape[0],))\n",
    "\n",
    "            # occluding_object = None\n",
    "            # for j in range(n_objects):\n",
    "            #     if i == j: continue\n",
    "            #     delt = -1 if i > j else 0\n",
    "            #     pose_estimates_without_j = jnp.concatenate((pose_estimates[:j], pose_estimates[j+1:]))\n",
    "            #     indices_without_j = indices[:j] + indices[j+1:]\n",
    "            #     depth_woj_wi = jax3dp3.render_multiobject(pose_estimates_without_j, indices_without_j)\n",
    "            #     depth_woj_woi = jax3dp3.render_multiobject(pose_estimates_without_j[:i+delt], indices_without_j[:i+delt])\n",
    "            #     if (jnp.sum(depth_woj_wi[:, :, 2] != depth_woj_woi[:, :, 2]) > occlusion_threshold):\n",
    "            #         occluding_object = j\n",
    "            #         break\n",
    "        else:\n",
    "            prior = jnp.ones((translation_deltas.shape[0],))\n",
    "\n",
    "        pose_estimate = pose_estimates[i]\n",
    "        translation_proposals = jnp.einsum(\n",
    "            \"ij,ajk->aik\", pose_estimate, translation_deltas\n",
    "        )\n",
    "        images = jax3dp3.render_parallel(translation_proposals, indices[i])\n",
    "        weights_new = scorer_parallel_jit(images, gt_image, prior)\n",
    "        best_weight_idx = jnp.argmax(weights_new)\n",
    "        pose_estimate = translation_proposals[best_weight_idx]\n",
    "        pose_estimates = pose_estimates.at[i].set(pose_estimate)\n",
    "\n",
    "    inferred_poses.append(pose_estimates.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_pos = pose_estimates[-1, :-1, 3]\n",
    "sorted_bowls = sorted(\n",
    "    [\n",
    "        pose_estimates[i, :-1, 3]\n",
    "        for i in range(len(indices))\n",
    "        if meshes[indices[i]] == \"bowl\"\n",
    "    ],\n",
    "    key=lambda x: x[0].item(),\n",
    ")\n",
    "closest_bowl_idx = jnp.argmin(\n",
    "    jnp.array([abs(apple_pos[0] - bowl_pos[0]) for bowl_pos in sorted_bowls])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:12<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output to: gravity_out.gif\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "    rgb_viz = Image.fromarray(rgb_images[t].astype(np.int8), mode=\"RGB\")\n",
    "    gt_depth_1 = get_depth_image(coord_images[t][:, :, 2], max=5.0)\n",
    "    poses = inferred_poses[t - start_t]\n",
    "    rendered_image = jax3dp3.render_multiobject(poses, indices)\n",
    "    rendered_image = get_depth_image(rendered_image[:, :, 2], max=5)\n",
    "\n",
    "    apple_pose = poses[-1]\n",
    "    rendered_apple = jax3dp3.render_single_object(apple_pose, indices[-1])\n",
    "    rendered_apple = get_depth_image(rendered_apple[:, :, 2], max=5)\n",
    "    all_images.append(\n",
    "        multi_panel(\n",
    "            [rgb_viz, gt_depth_1, rendered_image, rendered_apple],\n",
    "            [\n",
    "                f\"Class: {closest_bowl_idx}\\nRGB Image\",\n",
    "                f\"   Frame: {t}\\nActual Depth\",\n",
    "                \"\\nReconstructed Depth\",\n",
    "                \"\\nApple Only\",\n",
    "            ],\n",
    "            # [f\"RGB Image\", f\"Actual Depth\", \"Reconstructed Depth\", \"Reconstructed (Apple Only)\"],\n",
    "            middle_width=10,\n",
    "            label_fontsize=20,\n",
    "        )\n",
    "    )\n",
    "out_path = f\"{scene}_out.gif\"\n",
    "make_gif_from_pil_images(all_images, out_path)\n",
    "print(\"Saved output to:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. For each frame, enumerate the positions of new object poses (currently translation only), and for each object pick the pose that maximizes the 3DP3 liklihood under a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_steps = num_frames - start_t\n",
    "# num_steps = 2\n",
    "# occlusion_threshold = 10\n",
    "# inferred_poses = []\n",
    "# pose_estimates = init_poses.copy()\n",
    "# for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "#     gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "#     # New objects\n",
    "#     n_objects = pose_estimates.shape[0]\n",
    "#     for i in range(n_objects):\n",
    "#         # Occlusion detection: render depth with and without each object. If no difference, don't move object.\n",
    "#         depth_with_object = render_planes_multiobject_jit(pose_estimates)\n",
    "#         idxs = jnp.arange(n_objects) != i\n",
    "#         depth_without_object = render_planes_multiobject_multi_jit(\n",
    "#             pose_estimates[idxs],\n",
    "#             shape_planes[idxs],\n",
    "#             shape_dims[idxs],\n",
    "#         )\n",
    "\n",
    "#         if (\n",
    "#             jnp.sum(depth_with_object[:, :, 2] != depth_without_object[:, :, 2])\n",
    "#             < occlusion_threshold\n",
    "#         ):\n",
    "#             continue\n",
    "\n",
    "#         enumerations_full = jnp.tile(\n",
    "#             jnp.eye(4)[None, :, :],\n",
    "#             (enumerations.shape[0], pose_estimates.shape[0], 1, 1),\n",
    "#         )\n",
    "#         enumerations_full = enumerations_full.at[:, i, :, :].set(enumerations)\n",
    "#         proposals = jnp.einsum(\"bij,abjk->abik\", pose_estimates, enumerations_full)\n",
    "\n",
    "#         weights = batched_scorer_parallel_jit(proposals, gt_image)\n",
    "#         pose_estimates = proposals[weights.argmax()]\n",
    "#     inferred_poses.append(pose_estimates.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get the reconstructed poses for each frame and save them as a gif file with the gt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_images = []\n",
    "# for t in range(start_t, start_t + num_steps):\n",
    "#     rgb_viz = Image.fromarray(rgb_images[t].astype(np.int8), mode=\"RGB\")\n",
    "#     gt_depth_1 = get_depth_image(coord_images[t][:, :, 2], max=5.0)\n",
    "#     depth = render_planes_multiobject_jit(inferred_poses[t - start_t])\n",
    "#     depth = get_depth_image(depth[:, :, 2], max=5.0)\n",
    "#     all_images.append(\n",
    "#         multi_panel(\n",
    "#             [rgb_viz, gt_depth_1, depth],\n",
    "#             [f\"\\nRGB Image\", f\"   Frame: {t}\\nActual Depth\", \"\\nReconstructed Depth\"],\n",
    "#             middle_width=10,\n",
    "#             label_fontsize=20,\n",
    "#         )\n",
    "#     )\n",
    "# out_path = f\"{scene}_out.gif\"\n",
    "# make_gif_from_pil_images(all_images, out_path)\n",
    "# print(\"Saved output to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a3868bdd7d3c8a3e0bdbdcc5d56cecdac1cfc8e4c924f480e3352f5fc391e73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
