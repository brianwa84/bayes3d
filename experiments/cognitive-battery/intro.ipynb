{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitive Battery Introduction: Jax-3DP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jax3dp3.viz import save_depth_image, get_depth_image, multi_panel\n",
    "from jax3dp3.transforms_3d import (\n",
    "    transform_from_pos,\n",
    "    depth_to_coords_in_camera\n",
    ")\n",
    "from jax3dp3.jax_rendering import (\n",
    "    get_rectangular_prism_shape,\n",
    "    render_planes_multiobject,\n",
    "    batched_scorer_parallel_params,\n",
    ")\n",
    "from jax3dp3.likelihood import threedp3_likelihood\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from jax3dp3.enumerations import make_translation_grid_enumeration\n",
    "from jax3dp3.viz import make_gif_from_pil_images\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize camera metadata and path to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"addition\"\n",
    "data_path = f\"/home/khaledshehada/cog_jax3dp3_data/{scene}_data/videos/\"\n",
    "num_frames = len(os.listdir(os.path.join(data_path, \"frames\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_intrinsics(width, height, fov):\n",
    "    cx, cy = width / 2.0, height / 2.0\n",
    "    aspect_ratio = width / height\n",
    "    fov_y = np.deg2rad(fov)\n",
    "    fov_x = 2 * np.arctan(aspect_ratio * np.tan(fov_y / 2.0))\n",
    "    fx = cx / np.tan(fov_x / 2.0)\n",
    "    fy = cy / np.tan(fov_y / 2.0)\n",
    "\n",
    "    return fx, fy, cx, cy\n",
    "\n",
    "width = 300\n",
    "height = 300\n",
    "fov = 90\n",
    "\n",
    "if fov:\n",
    "    fx, fy, cx, cy = get_camera_intrinsics(width, height, fov)\n",
    "else:\n",
    "    fx = fy = cx = cy = 150\n",
    "\n",
    "fx_fy = jnp.array([fx, fy])\n",
    "cx_cy = jnp.array([cx, cy])\n",
    "\n",
    "K = jnp.array(\n",
    "    [\n",
    "        [fx_fy[0], 0.0, cx_cy[0]],\n",
    "        [0.0, fx_fy[1], cx_cy[1]],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load ground-truth RGB images, depth, and segmentation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_images, depth_images, seg_maps = [], [], []\n",
    "rgb_images_pil = []\n",
    "for i in range(num_frames):\n",
    "    rgb_path = os.path.join(data_path, f\"frames/frame_{i}.jpeg\")\n",
    "    rgb_img = Image.open(rgb_path)\n",
    "    rgb_images_pil.append(rgb_img)\n",
    "    rgb_images.append(np.array(rgb_img))\n",
    "\n",
    "    depth_path = os.path.join(data_path, f\"depths/frame_{i}.npy\")\n",
    "    depth_npy = np.load(depth_path)\n",
    "    depth_images.append(depth_npy)\n",
    "\n",
    "    seg_map = np.load(os.path.join(data_path, f\"segmented/frame_{i}.npy\"))\n",
    "    seg_maps.append(seg_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mask the depth and segmentation images to only include the relevant part of the scene (i.e. crop to the box above table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_images = []  # depth data in 2d view as images\n",
    "seg_images = []  # segmentation data as images\n",
    "\n",
    "for frame_idx in range(num_frames):\n",
    "    coord_image, _ = depth_to_coords_in_camera(depth_images[frame_idx], K)\n",
    "    segmentation_image = seg_maps[frame_idx].copy()\n",
    "    mask = np.invert(\n",
    "        (coord_image[:, :, 0] < 2.0)\n",
    "        * (coord_image[:, :, 0] > -1)\n",
    "        * (coord_image[:, :, 1] < 0.463)\n",
    "        * (coord_image[:, :, 1] > -0.8)\n",
    "        * (coord_image[:, :, 2] < 3)\n",
    "        * (coord_image[:, :, 2] > 0.25)\n",
    "    )\n",
    "    coord_image[mask, :] = 0.0\n",
    "    segmentation_image[mask, :] = 0.0\n",
    "    coord_images.append(coord_image)\n",
    "    seg_images.append(segmentation_image)\n",
    "\n",
    "coord_images = np.stack(coord_images)\n",
    "seg_images = np.stack(seg_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pick a starting frame and initialize the object shapes and poses from that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = 38\n",
    "seg_img = seg_images[start_t][:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ids = set()\n",
    "shape_planes, shape_dims, init_poses = [], [], []\n",
    "for obj_id in jnp.unique(seg_img):\n",
    "    if obj_id == 0:\n",
    "        continue\n",
    "    obj_mask = seg_img == obj_id\n",
    "    if scene == \"shape\" and np.sum(obj_mask) < 100:  # Only for shape, [TODO] delete or automate later\n",
    "        continue    \n",
    "\n",
    "    object_points = coord_images[start_t][obj_mask]\n",
    "    maxs = np.max(object_points, axis=0)\n",
    "    mins = np.min(object_points, axis=0)\n",
    "    dims = maxs - mins\n",
    "    center_of_box = (maxs + mins) / 2\n",
    "\n",
    "    init_pose = transform_from_pos(center_of_box)\n",
    "    init_poses.append(init_pose)\n",
    "\n",
    "    shape, dim = get_rectangular_prism_shape(dims)\n",
    "    shape_planes.append(shape)\n",
    "    shape_dims.append(dim)\n",
    "    obj_ids.add(obj_id.item())\n",
    "shape_planes = jnp.stack(shape_planes)\n",
    "shape_dims = jnp.stack(shape_dims)\n",
    "init_poses = jnp.stack(init_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save the reconstructed depths to check that they align with the actual depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=300x300>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def render_planes_multiobject_lambda(poses):\n",
    "    return render_planes_multiobject(\n",
    "        poses, shape_planes, shape_dims, height, width, fx, fy, cx, cy\n",
    "    )\n",
    "\n",
    "\n",
    "def render_planes_multiobject_multi_lambda(poses, planes, dims):\n",
    "    return render_planes_multiobject(poses, planes, dims, height, width, fx, fy, cx, cy)\n",
    "\n",
    "\n",
    "render_planes_multiobject_jit = jax.jit(render_planes_multiobject_lambda)\n",
    "render_planes_multiobject_multi_jit = jax.jit(render_planes_multiobject_multi_lambda)\n",
    "\n",
    "reconstruction_image = render_planes_multiobject_jit(init_poses)\n",
    "get_depth_image(reconstruction_image[:, :, 2], max=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the liklihood methods and the proposal enumerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liklihood parameters\n",
    "r = radius = 0.05\n",
    "outlier_prob = 0.01\n",
    "\n",
    "# Enumeration parameters\n",
    "n = 5  # number of enumerated proposals on each dimension (x, y, z).\n",
    "d = 0.1  # the minimum and maximum position delta on each dimension (x, y, z).\n",
    "\n",
    "# Liklihood methods\n",
    "def likelihood(x, params):\n",
    "    obs = params[0]\n",
    "    rendered_image = render_planes_multiobject(\n",
    "        x, shape_planes, shape_dims, height, width, fx, fy, cx, cy\n",
    "    )\n",
    "    weight = threedp3_likelihood(obs, rendered_image, r, outlier_prob)\n",
    "    return weight\n",
    "\n",
    "\n",
    "likelihood_parallel = jax.vmap(likelihood, in_axes=(0, None))\n",
    "batched_scorer_parallel_jit = jax.jit(\n",
    "    lambda poses, image: batched_scorer_parallel_params(\n",
    "        likelihood_parallel, n, poses, (image,)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Enumerating proposals\n",
    "enumerations = make_translation_grid_enumeration(-d, -d, -d, d, d, d, n, n, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. For each frame, enumerate the positions of new object poses (currently translation only), and for each object pick the pose that maximizes the 3DP3 liklihood under a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_estimates = init_poses.copy()\n",
    "t = start_t\n",
    "gt_image = jnp.array(coord_images[t])\n",
    "i = 0\n",
    "enumerations_full = jnp.tile(\n",
    "    jnp.eye(4)[None, :, :],\n",
    "    (enumerations.shape[0], pose_estimates.shape[0], 1, 1),\n",
    ")\n",
    "enumerations_full = enumerations_full.at[:, i, :, :].set(enumerations)\n",
    "proposals = jnp.einsum(\"bij,abjk->abik\", pose_estimates, enumerations_full)\n",
    "\n",
    "weights = batched_scorer_parallel_jit(proposals, gt_image)\n",
    "pose_estimates = proposals[weights.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:25<00:00,  5.81s/it]\n"
     ]
    }
   ],
   "source": [
    "num_steps = num_frames - start_t\n",
    "occlusion_threshold = 10\n",
    "inferred_poses = []\n",
    "pose_estimates = init_poses.copy()\n",
    "for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "    gt_image = jnp.array(coord_images[t])\n",
    "    n_objects = pose_estimates.shape[0]\n",
    "    \n",
    "    # New objects\n",
    "    seg_img = seg_images[t][:, :, 2]\n",
    "    new_init_poses, new_shape_planes, new_shape_dims = [], [], []\n",
    "    for obj_id in jnp.unique(seg_img):\n",
    "        if obj_id.item() in obj_ids or obj_id == 0:\n",
    "            continue\n",
    "            \n",
    "        obj_mask = seg_img == obj_id\n",
    "        if scene == \"shape\" and np.sum(obj_mask) < 100:  # Only for shape, [TODO] delete or automate later\n",
    "            continue    \n",
    "        \n",
    "        print(\"Found new object!\")\n",
    "        object_points = coord_images[start_t][obj_mask]\n",
    "        maxs = np.max(object_points, axis=0)\n",
    "        mins = np.min(object_points, axis=0)\n",
    "        dims = maxs - mins\n",
    "        center_of_box = (maxs + mins) / 2\n",
    "\n",
    "        init_pose = transform_from_pos(center_of_box)\n",
    "        new_init_poses.append(init_pose)\n",
    "\n",
    "        shape, dim = get_rectangular_prism_shape(dims)\n",
    "        new_shape_planes.append(shape)\n",
    "        new_shape_dims.append(dim)\n",
    "    \n",
    "    if new_init_poses:\n",
    "        pose_estimates = jnp.concatenate((pose_estimates, new_init_poses))\n",
    "        shape_planes = jnp.concatenate((shape_planes, new_shape_planes))\n",
    "        shape_dims = jnp.concatenate((shape_dims, new_shape_dims))\n",
    "    \n",
    "    for i in range(n_objects):\n",
    "        \n",
    "        # Occlusion detection: render depth with and without each object. If no difference, don't move object.\n",
    "        depth_with_object = render_planes_multiobject_jit(pose_estimates)\n",
    "        idxs = jnp.arange(n_objects) != i\n",
    "        depth_without_object = render_planes_multiobject_multi_jit(\n",
    "            pose_estimates[idxs],\n",
    "            shape_planes[idxs],\n",
    "            shape_dims[idxs],\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            jnp.sum(depth_with_object[:, :, 2] != depth_without_object[:, :, 2])\n",
    "            < occlusion_threshold\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        enumerations_full = jnp.tile(\n",
    "            jnp.eye(4)[None, :, :],\n",
    "            (enumerations.shape[0], pose_estimates.shape[0], 1, 1),\n",
    "        )\n",
    "        enumerations_full = enumerations_full.at[:, i, :, :].set(enumerations)\n",
    "        proposals = jnp.einsum(\"bij,abjk->abik\", pose_estimates, enumerations_full)\n",
    "\n",
    "        weights = batched_scorer_parallel_jit(proposals, gt_image)\n",
    "        pose_estimates = proposals[weights.argmax()]\n",
    "    inferred_poses.append(pose_estimates.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get the reconstructed poses for each frame and save them as a gif file with the gt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output to: addition_out.gif\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "for t in range(start_t, start_t + num_steps):\n",
    "    rgb_viz = Image.fromarray(rgb_images[t].astype(np.int8), mode=\"RGB\")\n",
    "    gt_depth_1 = get_depth_image(coord_images[t][:, :, 2], max=5.0)\n",
    "    depth = render_planes_multiobject_jit(inferred_poses[t - start_t])\n",
    "    depth = get_depth_image(depth[:, :, 2], max=5.0)\n",
    "    all_images.append(\n",
    "        multi_panel(\n",
    "            [rgb_viz, gt_depth_1, depth],\n",
    "            [f\"\\nRGB Image\", f\"   Frame: {t}\\nActual Depth\", \"\\nReconstructed Depth\"],\n",
    "            middle_width=10,\n",
    "            top_border=100,\n",
    "            fontsize=20,\n",
    "        )\n",
    "    )\n",
    "out_path = f\"{scene}_out.gif\"\n",
    "make_gif_from_pil_images(all_images, out_path)\n",
    "print(\"Saved output to:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a3868bdd7d3c8a3e0bdbdcc5d56cecdac1cfc8e4c924f480e3352f5fc391e73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
