{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitive Battery Introduction: Jax-3DP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax3dp3\n",
    "from jax3dp3.viz import (\n",
    "    save_depth_image,\n",
    "    get_depth_image,\n",
    "    multi_panel,\n",
    ")\n",
    "from jax3dp3.transforms_3d import transform_from_pos, depth_to_coords_in_camera\n",
    "from jax3dp3.jax_rendering import (\n",
    "    get_rectangular_prism_shape,\n",
    "    render_planes_multiobject,\n",
    "    batched_scorer_parallel_params,\n",
    ")\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from jax3dp3.viz import make_gif_from_pil_images\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cog_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize camera metadata and path to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"gravity\"\n",
    "data_path = f\"/home/khaledshehada/cog_jax3dp3_data/{scene}_data/videos/\"\n",
    "num_frames = len(os.listdir(os.path.join(data_path, \"frames\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing frame buffer size to (width, height, depth) = (320, 320, 2048)\n"
     ]
    }
   ],
   "source": [
    "width = 300\n",
    "height = 300\n",
    "fov = 90\n",
    "\n",
    "fx, fy, cx, cy = utils.get_camera_intrinsics(width, height, fov)\n",
    "\n",
    "fx_fy = jnp.array([fx, fy])\n",
    "cx_cy = jnp.array([cx, cy])\n",
    "near, far = 0.001, 50.0\n",
    "K = jnp.array(\n",
    "    [\n",
    "        [fx_fy[0], 0.0, cx_cy[0]],\n",
    "        [0.0, fx_fy[1], cx_cy[1]],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "jax3dp3.setup_renderer(height, width, fx, fy, cx, cy, near, far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load ground-truth RGB images, depth, and segmentation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_images, depth_images, seg_maps = [], [], []\n",
    "rgb_images_pil = []\n",
    "for i in range(num_frames):\n",
    "    rgb_path = os.path.join(data_path, f\"frames/frame_{i}.jpeg\")\n",
    "    if not os.path.isfile(rgb_path):\n",
    "        rgb_path = rgb_path.replace(\"jpeg\", \"png\")\n",
    "    rgb_img = Image.open(rgb_path)\n",
    "    rgb_images_pil.append(rgb_img)\n",
    "    rgb_images.append(np.array(rgb_img))\n",
    "\n",
    "    depth_path = os.path.join(data_path, f\"depths/frame_{i}.npy\")\n",
    "    depth_npy = np.load(depth_path)\n",
    "    depth_images.append(depth_npy)\n",
    "\n",
    "    seg_map = np.load(os.path.join(data_path, f\"segmented/frame_{i}.npy\"))\n",
    "    seg_maps.append(seg_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mask the depth and segmentation images to only include the relevant part of the scene (i.e. crop to the box above table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_images = []  # depth data in 2d view as images\n",
    "seg_images = []  # segmentation data as images\n",
    "\n",
    "for frame_idx in range(num_frames):\n",
    "    coord_image, _ = depth_to_coords_in_camera(depth_images[frame_idx], K)\n",
    "    segmentation_image = seg_maps[frame_idx].copy()\n",
    "    mask = np.invert(\n",
    "        # (coord_image[:, :, 0] < 2.0)\n",
    "        # * (coord_image[:, :, 0] > -1)\n",
    "        # * (coord_image[:, :, 1] < 0.463)\n",
    "        # * (coord_image[:, :, 1] > -0.8)\n",
    "        # * (coord_image[:, :, 2] < 3)\n",
    "        # * (coord_image[:, :, 2] > 0.25)\n",
    "        (coord_image[:, :, 0] < 1.1)\n",
    "        * (coord_image[:, :, 0] > -1)\n",
    "        * (coord_image[:, :, 1] < 0.565)\n",
    "        * (coord_image[:, :, 1] > -1)\n",
    "        * (coord_image[:, :, 2] > 1.2)\n",
    "        * (coord_image[:, :, 2] < 1.35)\n",
    "    )\n",
    "    coord_image[mask, :] = 0.0\n",
    "    segmentation_image[mask, :] = 0.0\n",
    "    coord_images.append(coord_image)\n",
    "    seg_images.append(segmentation_image)\n",
    "\n",
    "coord_images = np.stack(coord_images)\n",
    "seg_images = np.stack(seg_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "tube_s_long\n",
      "bowl\n",
      "tube_s\n"
     ]
    }
   ],
   "source": [
    "# Load meshes\n",
    "meshes = []\n",
    "meshes_path = \"/home/khaledshehada/cog_jax3dp3_data/gravity_data/meshes/\"\n",
    "for mesh_name in os.listdir(meshes_path):\n",
    "    mesh_path = os.path.join(meshes_path, mesh_name)\n",
    "    mesh = trimesh.load(mesh_path)\n",
    "    jax3dp3.load_model(mesh)\n",
    "    meshes.append(mesh_name.replace(\".obj\", \"\"))\n",
    "    print(meshes[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pick a starting frame and initialize the object shapes and poses from that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_t = 36\n",
    "seg_img = seg_images[start_t]\n",
    "Image.fromarray(seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] 252201\n",
      "[146  40   8] 6464\n",
      "[18 40 64] 7596\n",
      "[18 40  0] 91119\n",
      "[16 96 72] 3856\n",
      "[130  96  73] 3832\n",
      "[146  96  72] 5063\n",
      "[ 0 96 73] 87372\n",
      "[18 96 65] 6132\n",
      "[144  96   9] 3361\n",
      "[ 2 96  1] 3344\n",
      "[128  96  65] 3683\n",
      "[16 32  9] 721\n",
      "[73 68 32] 66\n",
      "[145  68   8] 1280\n",
      "[10 68 32] 63\n",
      "[25 48 32] 80\n",
      "[97 48 68] 77\n",
      "[24 48 68] 76\n",
      "[88 48 64] 1271\n",
      "[96 48 36] 59\n",
      "[32 48 68] 75\n",
      "[33 48 64] 1270\n",
      "[89 48 68] 75\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=300x300>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_objects = 12\n",
    "indices, poses = [], []\n",
    "obj_ids = jnp.unique(seg_img.reshape(-1, 3), axis=0)\n",
    "obj_ids = sorted(\n",
    "    obj_ids,\n",
    "    key = lambda x: jnp.all(seg_img == x, axis=-1).sum(),\n",
    "    reverse = True\n",
    ")\n",
    "for obj_id in obj_ids[:num_objects + 1]:\n",
    "    if jnp.all(obj_id == 0):\n",
    "        # Background\n",
    "        continue\n",
    "    \n",
    "    obj_mask = jnp.all(seg_img == obj_id, axis=-1)\n",
    "    masked_depth = coord_images[start_t].copy()\n",
    "    masked_depth[~obj_mask] = 0\n",
    "\n",
    "    object_points = coord_images[start_t][obj_mask]\n",
    "    maxs = np.max(object_points, axis=0)\n",
    "    mins = np.min(object_points, axis=0)\n",
    "    dims = maxs - mins\n",
    "    obj_center = (maxs + mins) / 2\n",
    "    obj_transform = transform_from_pos(obj_center)\n",
    "\n",
    "    best = None\n",
    "    k = np.inf\n",
    "    for m in range(len(meshes)):\n",
    "        obj_transforms = utils.get_object_transforms(meshes[m], obj_transform)\n",
    "        for i, transform in enumerate(obj_transforms):\n",
    "            rendered_image = jax3dp3.render_single_object(transform, m)\n",
    "            keep_points = jnp.sum(\n",
    "                jnp.logical_or(\n",
    "                    ((masked_depth[:, :, 2] != 0.0) * (rendered_image[:, :, 2] == 0)),\n",
    "                    ((masked_depth[:, :, 2] == 0.0) * (rendered_image[:, :, 2] != 0)),\n",
    "                )\n",
    "            ) / (rendered_image[:, :, 2] != 0.0).sum()\n",
    "            if keep_points < k:\n",
    "                k = keep_points\n",
    "                best = (m, transform)\n",
    "    if best:\n",
    "        indices.append(best[0])\n",
    "        poses.append(best[1])\n",
    "\n",
    "rendered_image = jax3dp3.render_multiobject(jnp.array(poses), indices)\n",
    "get_depth_image(rendered_image[:, :, 2], max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=300x300>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax3dp3.viz.get_depth_image(coord_images[start_t][:, :, 2], max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_images[start_t].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ids = set()\n",
    "shape_planes, shape_dims, init_poses = [], [], []\n",
    "for obj_id in jnp.unique(seg_img):\n",
    "    obj_mask = jnp.all(seg_img == obj_id, axis=-1)\n",
    "    if obj_id == 0:\n",
    "        continue\n",
    "    # if scene == \"shape\" and np.sum(obj_mask) < 100:  # Only for shape, [TODO] delete or automate later\n",
    "    #     continue\n",
    "    if jnp.all(obj_id == 0) or seg_img[obj_mask].shape[0] < 15:\n",
    "        continue\n",
    "\n",
    "    object_points = coord_images[start_t][obj_mask]\n",
    "    maxs = np.max(object_points, axis=0)\n",
    "    mins = np.min(object_points, axis=0)\n",
    "    dims = maxs - mins\n",
    "    center_of_box = (maxs + mins) / 2\n",
    "\n",
    "    init_pose = transform_from_pos(center_of_box)\n",
    "    init_poses.append(init_pose)\n",
    "\n",
    "    shape, dim = get_rectangular_prism_shape(dims)\n",
    "    shape_planes.append(shape)\n",
    "    shape_dims.append(dim)\n",
    "    obj_ids.add(obj_id.item())\n",
    "shape_planes = jnp.stack(shape_planes)\n",
    "shape_dims = jnp.stack(shape_dims)\n",
    "init_poses = jnp.stack(init_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save the reconstructed depths to check that they align with the actual depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_planes_multiobject_lambda(poses):\n",
    "    return render_planes_multiobject(\n",
    "        poses, shape_planes, shape_dims, height, width, fx, fy, cx, cy\n",
    "    )\n",
    "\n",
    "\n",
    "def render_planes_multiobject_multi_lambda(poses, planes, dims):\n",
    "    return render_planes_multiobject(poses, planes, dims, height, width, fx, fy, cx, cy)\n",
    "\n",
    "\n",
    "render_planes_multiobject_jit = jax.jit(render_planes_multiobject_lambda)\n",
    "render_planes_multiobject_multi_jit = jax.jit(render_planes_multiobject_multi_lambda)\n",
    "\n",
    "reconstruction_image = render_planes_multiobject_jit(init_poses)\n",
    "get_depth_image(reconstruction_image[:, :, 2], max=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the liklihood methods and the proposal enumerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liklihood parameters\n",
    "r = radius = 0.05\n",
    "outlier_prob = 0.01\n",
    "outlier_volume = 1\n",
    "\n",
    "# Enumeration parameters\n",
    "n = 5  # number of enumerated proposals on each dimension (x, y, z).\n",
    "d = 0.1  # the minimum and maximum position delta on each dimension (x, y, z).\n",
    "\n",
    "# Liklihood methods\n",
    "def likelihood(x, params):\n",
    "    obs = params[0]\n",
    "    rendered_image = render_planes_multiobject(\n",
    "        x, shape_planes, shape_dims, height, width, fx, fy, cx, cy\n",
    "    )\n",
    "    weight = jax3dp3.threedp3_likelihood(\n",
    "        obs, rendered_image, r, outlier_prob, outlier_volume\n",
    "    )\n",
    "    return weight\n",
    "\n",
    "\n",
    "likelihood_parallel = jax.vmap(likelihood, in_axes=(0, None))\n",
    "batched_scorer_parallel_jit = jax.jit(\n",
    "    lambda poses, image: batched_scorer_parallel_params(\n",
    "        likelihood_parallel, n, poses, (image,)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Enumerating proposals\n",
    "enumerations = jax3dp3.make_translation_grid_enumeration(-d, -d, -d, d, d, d, n, n, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. For each frame, enumerate the positions of new object poses (currently translation only), and for each object pick the pose that maximizes the 3DP3 liklihood under a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_estimates = init_poses.copy()\n",
    "t = start_t\n",
    "gt_image = jnp.array(coord_images[t])\n",
    "i = 0\n",
    "enumerations_full = jnp.tile(\n",
    "    jnp.eye(4)[None, :, :],\n",
    "    (enumerations.shape[0], pose_estimates.shape[0], 1, 1),\n",
    ")\n",
    "enumerations_full = enumerations_full.at[:, i, :, :].set(enumerations)\n",
    "proposals = jnp.einsum(\"bij,abjk->abik\", pose_estimates, enumerations_full)\n",
    "\n",
    "weights = batched_scorer_parallel_jit(proposals, gt_image)\n",
    "pose_estimates = proposals[weights.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New objects\n",
    "# gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "# depth_full = render_planes_multiobject_jit(pose_estimates)[:,:,2]\n",
    "# depth_difference = gt_image.at[jnp.where(depth_full != 0)].set(0)\n",
    "\n",
    "# depth_difference = depth_difference.reshape(-1, 3)\n",
    "# clustering = KMeans().fit(gt_image.reshape(-1, 3))\n",
    "# labels = clustering.labels_.reshape(300, 300)\n",
    "\n",
    "# for label in range(labels.max()):\n",
    "#     gt_image = jnp.array(coord_images[t]).at[labels != label].set(0)\n",
    "#     save_depth_image(gt_image[:,:,2], f\"clustering/{label}.png\", max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = num_frames - start_t\n",
    "num_steps = 2\n",
    "occlusion_threshold = 10\n",
    "inferred_poses = []\n",
    "pose_estimates = init_poses.copy()\n",
    "for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "    gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "    # New objects\n",
    "    n_objects = pose_estimates.shape[0]\n",
    "    for i in range(n_objects):\n",
    "        # Occlusion detection: render depth with and without each object. If no difference, don't move object.\n",
    "        depth_with_object = render_planes_multiobject_jit(pose_estimates)\n",
    "        idxs = jnp.arange(n_objects) != i\n",
    "        depth_without_object = render_planes_multiobject_multi_jit(\n",
    "            pose_estimates[idxs],\n",
    "            shape_planes[idxs],\n",
    "            shape_dims[idxs],\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            jnp.sum(depth_with_object[:, :, 2] != depth_without_object[:, :, 2])\n",
    "            < occlusion_threshold\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        enumerations_full = jnp.tile(\n",
    "            jnp.eye(4)[None, :, :],\n",
    "            (enumerations.shape[0], pose_estimates.shape[0], 1, 1),\n",
    "        )\n",
    "        enumerations_full = enumerations_full.at[:, i, :, :].set(enumerations)\n",
    "        proposals = jnp.einsum(\"bij,abjk->abik\", pose_estimates, enumerations_full)\n",
    "\n",
    "        weights = batched_scorer_parallel_jit(proposals, gt_image)\n",
    "        pose_estimates = proposals[weights.argmax()]\n",
    "    inferred_poses.append(pose_estimates.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get the reconstructed poses for each frame and save them as a gif file with the gt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for t in range(start_t, start_t + num_steps):\n",
    "    rgb_viz = Image.fromarray(rgb_images[t].astype(np.int8), mode=\"RGB\")\n",
    "    gt_depth_1 = get_depth_image(coord_images[t][:, :, 2], max=5.0)\n",
    "    depth = render_planes_multiobject_jit(inferred_poses[t - start_t])\n",
    "    depth = get_depth_image(depth[:, :, 2], max=5.0)\n",
    "    all_images.append(\n",
    "        multi_panel(\n",
    "            [rgb_viz, gt_depth_1, depth],\n",
    "            [f\"\\nRGB Image\", f\"   Frame: {t}\\nActual Depth\", \"\\nReconstructed Depth\"],\n",
    "            middle_width=10,\n",
    "            label_fontsize=20,\n",
    "        )\n",
    "    )\n",
    "out_path = f\"{scene}_out.gif\"\n",
    "make_gif_from_pil_images(all_images, out_path)\n",
    "print(\"Saved output to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a3868bdd7d3c8a3e0bdbdcc5d56cecdac1cfc8e4c924f480e3352f5fc391e73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
