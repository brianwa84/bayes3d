{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitive Battery Introduction: Jax-3DP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7000/static/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax3dp3\n",
    "from jax3dp3.viz import (\n",
    "    save_depth_image,\n",
    "    get_depth_image,\n",
    "    multi_panel,\n",
    ")\n",
    "from jax3dp3.transforms_3d import (\n",
    "    transform_from_pos,\n",
    "    depth_to_coords_in_camera,\n",
    "    apply_transform,\n",
    "    point_cloud_image_to_points,\n",
    ")\n",
    "from jax3dp3.jax_rendering import (\n",
    "    get_rectangular_prism_shape,\n",
    "    render_planes_multiobject,\n",
    "    batched_scorer_parallel_params,\n",
    ")\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from jax3dp3.viz import make_gif_from_pil_images\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cog_utils as utils\n",
    "\n",
    "jax3dp3.setup_visualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize camera metadata and path to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"gravity\"\n",
    "data_path = f\"/home/khaledshehada/cog_jax3dp3_data/{scene}_data/videos/\"\n",
    "num_frames = len(os.listdir(os.path.join(data_path, \"frames\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing frame buffer size to (width, height, depth) = (320, 320, 2048)\n"
     ]
    }
   ],
   "source": [
    "width = 300\n",
    "height = 300\n",
    "fov = 90\n",
    "\n",
    "fx, fy, cx, cy = utils.get_camera_intrinsics(width, height, fov)\n",
    "\n",
    "fx_fy = jnp.array([fx, fy])\n",
    "cx_cy = jnp.array([cx, cy])\n",
    "near, far = 0.001, 50.0\n",
    "K = jnp.array(\n",
    "    [\n",
    "        [fx_fy[0], 0.0, cx_cy[0]],\n",
    "        [0.0, fx_fy[1], cx_cy[1]],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "jax3dp3.setup_renderer(height, width, fx, fy, cx, cy, near, far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load ground-truth RGB images, depth, and segmentation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_images, depth_images, seg_maps = [], [], []\n",
    "rgb_images_pil = []\n",
    "for i in range(num_frames):\n",
    "    rgb_path = os.path.join(data_path, f\"frames/frame_{i}.jpeg\")\n",
    "    if not os.path.isfile(rgb_path):\n",
    "        rgb_path = rgb_path.replace(\"jpeg\", \"png\")\n",
    "    rgb_img = Image.open(rgb_path)\n",
    "    rgb_images_pil.append(rgb_img)\n",
    "    rgb_images.append(np.array(rgb_img))\n",
    "\n",
    "    depth_path = os.path.join(data_path, f\"depths/frame_{i}.npy\")\n",
    "    depth_npy = np.load(depth_path)\n",
    "    depth_images.append(depth_npy)\n",
    "\n",
    "    seg_map = np.load(os.path.join(data_path, f\"segmented/frame_{i}.npy\"))\n",
    "    seg_maps.append(seg_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mask the depth and segmentation images to only include the relevant part of the scene (i.e. crop to the box above table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_images = []  # depth data in 2d view as images\n",
    "seg_images = []  # segmentation data as images\n",
    "\n",
    "for frame_idx in range(num_frames):\n",
    "    coord_image, _ = depth_to_coords_in_camera(depth_images[frame_idx], K)\n",
    "    segmentation_image = seg_maps[frame_idx].copy()\n",
    "    mask = np.invert(\n",
    "        # (coord_image[:, :, 0] < 2.0)\n",
    "        # * (coord_image[:, :, 0] > -1)\n",
    "        # * (coord_image[:, :, 1] < 0.463)\n",
    "        # * (coord_image[:, :, 1] > -0.8)\n",
    "        # * (coord_image[:, :, 2] < 3)\n",
    "        # * (coord_image[:, :, 2] > 0.25)\n",
    "        (coord_image[:, :, 0] < 1.1)\n",
    "        * (coord_image[:, :, 0] > -1)\n",
    "        * (coord_image[:, :, 1] < 0.565)\n",
    "        * (coord_image[:, :, 1] > -1)\n",
    "        * (coord_image[:, :, 2] > 1.2)\n",
    "        * (coord_image[:, :, 2] < 1.35)\n",
    "    )\n",
    "    coord_image[mask, :] = 0.0\n",
    "    segmentation_image[mask, :] = 0.0\n",
    "    coord_images.append(coord_image)\n",
    "    seg_images.append(segmentation_image)\n",
    "\n",
    "coord_images = np.stack(coord_images)\n",
    "seg_images = np.stack(seg_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pick a starting frame and initialize the object shapes and poses from that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_t = 36\n",
    "seg_img = seg_images[start_t]\n",
    "Image.fromarray(seg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "QH6239 Qhull precision error: initial Delaunay input sites are cocircular or cospherical.  Use option 'Qz' for the Delaunay triangulation or Voronoi diagram of cocircular/cospherical points; it adds a point \"at infinity\".  Alternatively use option 'QJ' to joggle the input.  Use option 'Qs' to search all points for the initial simplex.\n\ninput sites with last coordinate projected to a paraboloid\n- p1(v5):  0.18 -0.93   2.7   2.5\n- p2(v4):  0.19 -0.93   2.7   2.7\n- p10(v3):  0.17  -0.9   2.7     0\n- p9(v2):   0.2 -0.91   2.7   1.3\n- p0(v1):  0.17 -0.93   2.7   2.4\n\nWhile executing:  | qhull d Qbb Qt\nOptions selected for Qhull 2020.2.r 2020/08/31:\n  run-id 1668612891  delaunay  Qbbound-last  Qtriangulate  _pre-merge\n  _zero-centrum  Pgood  _max-width 0.027  Error-roundoff 5.4e-15\n  _one-merge 4.8e-14  _near-inside 2.4e-13  Visible-distance 3.2e-14\n  U-max-coplanar 3.2e-14  Width-outside 6.5e-14  _wide-facet 1.9e-13\n  _maxoutside 6.5e-14\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4792/2429117797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_cloud_image_to_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mclouds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mlearned_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax3dp3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_alpha_mesh_from_point_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmeshes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearned_mesh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jax3dp3/jax3dp3/mesh.py\u001b[0m in \u001b[0;36mmake_alpha_mesh_from_point_cloud\u001b[0;34m(point_cloud, alpha)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mpcd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPointCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mpcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVector3dVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlearned_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTriangleMesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_from_point_cloud_alpha_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mlearned_mesh_trimesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrimesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearned_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearned_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlearned_mesh_trimesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: QH6239 Qhull precision error: initial Delaunay input sites are cocircular or cospherical.  Use option 'Qz' for the Delaunay triangulation or Voronoi diagram of cocircular/cospherical points; it adds a point \"at infinity\".  Alternatively use option 'QJ' to joggle the input.  Use option 'Qs' to search all points for the initial simplex.\n\ninput sites with last coordinate projected to a paraboloid\n- p1(v5):  0.18 -0.93   2.7   2.5\n- p2(v4):  0.19 -0.93   2.7   2.7\n- p10(v3):  0.17  -0.9   2.7     0\n- p9(v2):   0.2 -0.91   2.7   1.3\n- p0(v1):  0.17 -0.93   2.7   2.4\n\nWhile executing:  | qhull d Qbb Qt\nOptions selected for Qhull 2020.2.r 2020/08/31:\n  run-id 1668612891  delaunay  Qbbound-last  Qtriangulate  _pre-merge\n  _zero-centrum  Pgood  _max-width 0.027  Error-roundoff 5.4e-15\n  _one-merge 4.8e-14  _near-inside 2.4e-13  Visible-distance 3.2e-14\n  U-max-coplanar 3.2e-14  Width-outside 6.5e-14  _wide-facet 1.9e-13\n  _maxoutside 6.5e-14\n"
     ]
    }
   ],
   "source": [
    "num_objects = 12\n",
    "indices, init_poses = [], []\n",
    "obj_ids = jnp.unique(seg_img.reshape(-1, 3), axis=0)\n",
    "obj_ids = sorted(\n",
    "    obj_ids, key=lambda x: jnp.all(seg_img == x, axis=-1).sum(), reverse=True\n",
    ")\n",
    "clouds = []\n",
    "meshes = []\n",
    "for obj_id in obj_ids[num_objects : num_objects + 1]:\n",
    "    if jnp.all(obj_id == 0):\n",
    "        # Background\n",
    "        continue\n",
    "\n",
    "    obj_mask = jnp.all(seg_img == obj_id, axis=-1)\n",
    "    masked_depth = coord_images[start_t].copy()\n",
    "    masked_depth[~obj_mask] = 0\n",
    "\n",
    "    object_points = coord_images[start_t][obj_mask]\n",
    "    maxs = np.max(object_points, axis=0)\n",
    "    mins = np.min(object_points, axis=0)\n",
    "    dims = maxs - mins\n",
    "    obj_center = (maxs + mins) / 2\n",
    "    obj_transform = transform_from_pos(obj_center)\n",
    "\n",
    "    cloud = apply_transform(point_cloud_image_to_points(masked_depth), obj_transform)\n",
    "    clouds.append(cloud)\n",
    "    learned_mesh = jax3dp3.mesh.make_alpha_mesh_from_point_cloud(cloud, 0.001)\n",
    "    meshes.append(learned_mesh)\n",
    "\n",
    "for i, m in enumerate(meshes):\n",
    "    jax3dp3.show_trimesh(f\"mesh_{i}\", m)\n",
    "\n",
    "# jax3dp3.show_cloud(\"1\", np.vstack(clouds))\n",
    "\n",
    "#\n",
    "# init_poses = jnp.array(init_poses)\n",
    "# rendered_image = jax3dp3.render_multiobject(init_poses, indices)\n",
    "# get_depth_image(rendered_image[:, :, 2], max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the liklihood methods and the proposal enumerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liklihood parameters\n",
    "r = radius = 0.1\n",
    "outlier_prob = 0.01\n",
    "outlier_volume = 10\n",
    "\n",
    "# Enumeration parameters\n",
    "n = 5  # number of enumerated proposals on each dimension (x, y, z).\n",
    "d = 0.03  # the minimum and maximum position delta on each dimension (x, y, z).\n",
    "\n",
    "\n",
    "def scorer(rendered_image, gt):\n",
    "\n",
    "    weight = jax3dp3.likelihood.threedp3_likelihood(\n",
    "        gt, rendered_image, r, outlier_prob, outlier_volume\n",
    "    )\n",
    "    return weight\n",
    "\n",
    "\n",
    "scorer_parallel = jax.vmap(scorer, in_axes=(0, None))\n",
    "scorer_parallel_jit = jax.jit(scorer_parallel)\n",
    "\n",
    "# Enumerating proposals\n",
    "translation_deltas = jax3dp3.make_translation_grid_enumeration(\n",
    "    -d, -d, -d, d, d, d, n, n, n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_estimates = init_poses.copy()\n",
    "t = start_t\n",
    "gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "translation_deltas_full = jnp.tile(\n",
    "    jnp.eye(4)[None, :, :],\n",
    "    (translation_deltas.shape[0], pose_estimates.shape[0], 1, 1),\n",
    ")\n",
    "translation_deltas_full = translation_deltas_full.at[:, 0, :, :].set(translation_deltas)\n",
    "translation_proposals = jnp.einsum(\n",
    "    \"bij,abjk->abik\", pose_estimates, translation_deltas_full\n",
    ")\n",
    "images = jax3dp3.render_parallel(translation_proposals, 0)\n",
    "weights_new = scorer_parallel_jit(images, gt_image)\n",
    "pose_estimates = translation_proposals[jnp.argmax(weights_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = num_frames - start_t\n",
    "num_steps = 100\n",
    "inferred_poses = []\n",
    "pose_estimates = init_poses.copy()\n",
    "for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "    gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "    # New objects\n",
    "    n_objects = pose_estimates.shape[0]\n",
    "    for i in range(n_objects):\n",
    "        pose_estimate = pose_estimates[i]\n",
    "        translation_proposals = jnp.einsum(\n",
    "            \"ij,ajk->aik\", pose_estimate, translation_deltas\n",
    "        )\n",
    "        images = jax3dp3.render_parallel(translation_proposals, indices[i])\n",
    "        weights_new = scorer_parallel_jit(images, gt_image)\n",
    "        if t == start_t or (weights_new.max() - weights_new.min() > 10):\n",
    "            best_weight_idx = jnp.argmax(weights_new)\n",
    "        else:\n",
    "            print(weights_new.max() - weights_new.min())\n",
    "        pose_estimate = translation_proposals[best_weight_idx]\n",
    "        pose_estimates = pose_estimates.at[i].set(pose_estimate)\n",
    "\n",
    "    inferred_poses.append(pose_estimates.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for t in range(start_t, start_t + num_steps):\n",
    "    rgb_viz = Image.fromarray(rgb_images[t].astype(np.int8), mode=\"RGB\")\n",
    "    gt_depth_1 = get_depth_image(coord_images[t][:, :, 2], max=5.0)\n",
    "    poses = inferred_poses[t - start_t]\n",
    "    rendered_image = jax3dp3.render_multiobject(poses, indices)\n",
    "    rendered_image = get_depth_image(rendered_image[:, :, 2], max=5)\n",
    "\n",
    "    apple_pose = poses[-1]\n",
    "    rendered_apple = jax3dp3.render_single_object(apple_pose, indices[-1])\n",
    "    rendered_apple = get_depth_image(rendered_apple[:, :, 2], max=5)\n",
    "    all_images.append(\n",
    "        multi_panel(\n",
    "            [rgb_viz, gt_depth_1, rendered_image, rendered_apple],\n",
    "            [\n",
    "                f\"\\nRGB Image\",\n",
    "                f\"   Frame: {t}\\nActual Depth\",\n",
    "                \"\\nReconstructed Depth\",\n",
    "                \"\\nApple Only\",\n",
    "            ],\n",
    "            middle_width=10,\n",
    "            label_fontsize=20,\n",
    "        )\n",
    "    )\n",
    "out_path = f\"{scene}_out.gif\"\n",
    "make_gif_from_pil_images(all_images, out_path)\n",
    "print(\"Saved output to:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. For each frame, enumerate the positions of new object poses (currently translation only), and for each object pick the pose that maximizes the 3DP3 liklihood under a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_steps = num_frames - start_t\n",
    "# num_steps = 2\n",
    "# occlusion_threshold = 10\n",
    "# inferred_poses = []\n",
    "# pose_estimates = init_poses.copy()\n",
    "# for t in tqdm(range(start_t, start_t + num_steps)):\n",
    "#     gt_image = jnp.array(coord_images[t])\n",
    "\n",
    "#     # New objects\n",
    "#     n_objects = pose_estimates.shape[0]\n",
    "#     for i in range(n_objects):\n",
    "#         # Occlusion detection: render depth with and without each object. If no difference, don't move object.\n",
    "#         depth_with_object = render_planes_multiobject_jit(pose_estimates)\n",
    "#         idxs = jnp.arange(n_objects) != i\n",
    "#         depth_without_object = render_planes_multiobject_multi_jit(\n",
    "#             pose_estimates[idxs],\n",
    "#             shape_planes[idxs],\n",
    "#             shape_dims[idxs],\n",
    "#         )\n",
    "\n",
    "#         if (\n",
    "#             jnp.sum(depth_with_object[:, :, 2] != depth_without_object[:, :, 2])\n",
    "#             < occlusion_threshold\n",
    "#         ):\n",
    "#             continue\n",
    "\n",
    "#         enumerations_full = jnp.tile(\n",
    "#             jnp.eye(4)[None, :, :],\n",
    "#             (enumerations.shape[0], pose_estimates.shape[0], 1, 1),\n",
    "#         )\n",
    "#         enumerations_full = enumerations_full.at[:, i, :, :].set(enumerations)\n",
    "#         proposals = jnp.einsum(\"bij,abjk->abik\", pose_estimates, enumerations_full)\n",
    "\n",
    "#         weights = batched_scorer_parallel_jit(proposals, gt_image)\n",
    "#         pose_estimates = proposals[weights.argmax()]\n",
    "#     inferred_poses.append(pose_estimates.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Get the reconstructed poses for each frame and save them as a gif file with the gt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_images = []\n",
    "# for t in range(start_t, start_t + num_steps):\n",
    "#     rgb_viz = Image.fromarray(rgb_images[t].astype(np.int8), mode=\"RGB\")\n",
    "#     gt_depth_1 = get_depth_image(coord_images[t][:, :, 2], max=5.0)\n",
    "#     depth = render_planes_multiobject_jit(inferred_poses[t - start_t])\n",
    "#     depth = get_depth_image(depth[:, :, 2], max=5.0)\n",
    "#     all_images.append(\n",
    "#         multi_panel(\n",
    "#             [rgb_viz, gt_depth_1, depth],\n",
    "#             [f\"\\nRGB Image\", f\"   Frame: {t}\\nActual Depth\", \"\\nReconstructed Depth\"],\n",
    "#             middle_width=10,\n",
    "#             label_fontsize=20,\n",
    "#         )\n",
    "#     )\n",
    "# out_path = f\"{scene}_out.gif\"\n",
    "# make_gif_from_pil_images(all_images, out_path)\n",
    "# print(\"Saved output to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a3868bdd7d3c8a3e0bdbdcc5d56cecdac1cfc8e4c924f480e3352f5fc391e73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
